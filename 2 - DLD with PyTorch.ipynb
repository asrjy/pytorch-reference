{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9f94e1",
   "metadata": {},
   "source": [
    "stage 1:\n",
    "data preperation stage\n",
    "\n",
    "- prepare the data. load from external sourec and convert it to appropriate format for model training. \n",
    "- load this data and convert it to numeric values in the form of tensors. \n",
    "- these tensors act as inputs to the model during the training stage. \n",
    "- in training, they are preprocessed via transforms and grouped into batches.\n",
    "\n",
    "stage 2:\n",
    "experimentation and development\n",
    "- train model, test performance and optimize our hyperparameters to improve performance to desired level. \n",
    "- pytorch provides modules in torch.nn module to help create and train neural networks. \n",
    "- we define loss function and select an optimizer\n",
    "- backprop is performed and model parameters are updated. \n",
    "- within each epoch, we also validate our model using validation data.\n",
    "- finally model is passed to test data and performance is measured. \n",
    "\n",
    "stage 3:\n",
    "model deployment \n",
    "- fully trained model can be put on pytorch hub or edge device or local server\n",
    "- it may also be put on cloud server after some postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9822b2cf",
   "metadata": {},
   "source": [
    "- pytorch provides built in classes and utilities such as Dataset, DataLoader and Sampler classes for loading various types of data. \n",
    "- Dataset class defines how to access and preprocess data from file or data sources. \n",
    "- Sampler class defines how to sample data from dataset  in order to create batches. \n",
    "- DataLoader class combines dataset with a sampler and allows you to iterate over a set of batches. \n",
    "- pytorch libraries such as Torchvision and Torchtext provide classes to support computer vision data and natural language data. \n",
    "- torchvision.datasets module provides number of subclasses to load image data from popular academic datasets. \n",
    "- CIFAR-10 consists of 50k training and 10k testing for 10 possible objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f00f9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "train_data = CIFAR10(root = './train', train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e5fa55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./train\n",
      "    Split: Train\n"
     ]
    }
   ],
   "source": [
    "# printing general information\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c1dd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "# number of data samples\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea0630c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.data.shape)\n",
    "# 50000 images, each is 32x32 with 3 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9200f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 9, 9, 4, 1, 1, 2, 7, 8, 3]\n"
     ]
    }
   ],
   "source": [
    "print(train_data.targets[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de6676d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "print(train_data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8472070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}\n"
     ]
    }
   ],
   "source": [
    "print(train_data.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aba6f20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "620ae305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc0418d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10a26b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33a50f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=RGB size=32x32 at 0x7F59A8430F90>\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7d1385",
   "metadata": {},
   "source": [
    "PIL is a common image format that uses Pillow Library to store image pixel values in the format of height x width x channels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7a0f185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(type(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "436f572b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2721cd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frog\n"
     ]
    }
   ],
   "source": [
    "print(train_data.classes[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5050deb",
   "metadata": {},
   "source": [
    "- loading the test data\n",
    "- changing root folder and setting train = False will work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c41bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CIFAR10(root = './test/', train = False, download = True)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7831a143",
   "metadata": {},
   "source": [
    "- data needs to be adjusted before it's passed to the NN model.\n",
    "- adjustments include normalization, type conversion from object to tensor etc.,\n",
    "- can be perfomed by applying transforms. \n",
    "- in pytorch, a sequence of transforms can be defined and can be applied later when data is accessed. \n",
    "- can be performed using cpu and gpu parallelly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7595aea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding = 4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "            mean = (0.4914, 0.4822, 0.4465),\n",
    "            std = (0.2023, 0.1994, 0.2010)\n",
    "    )\n",
    "])\n",
    "\n",
    "train_data = CIFAR10(root = './train', train = True, download = True, transform = train_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57042e75",
   "metadata": {},
   "source": [
    "- mean and standard deviation numbers were predetermined based on the dataset. \n",
    "- transform method is used when creating the dataset itself. \n",
    "- transforms are defined using transform.Compose() method. \n",
    "- transforms used above are randomly cropping images, randomly flipping images, converting them to tensors, normalizing them to predetermined means and standard deviations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a96138cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./train\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomCrop(size=(32, 32), padding=4)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70ac727e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomCrop(size=(32, 32), padding=4)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(train_data.transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "518b8be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([3, 32, 32])\n",
      "tensor([[[-2.4291e+00, -2.4291e+00, -2.4291e+00,  ..., -2.4291e+00,\n",
      "          -2.4291e+00, -2.4291e+00],\n",
      "         [-1.2854e+00, -1.5955e+00, -1.4598e+00,  ...,  6.3375e-01,\n",
      "           5.1744e-01,  4.3990e-01],\n",
      "         [-2.1189e+00, -2.4291e+00, -2.0801e+00,  ..., -4.4721e-02,\n",
      "          -1.2226e-01, -6.4106e-02],\n",
      "         ...,\n",
      "         [ 1.8356e+00,  1.4673e+00,  1.1765e+00,  ...,  9.2452e-01,\n",
      "          -1.3435e+00, -1.7894e+00],\n",
      "         [ 1.6030e+00,  1.4673e+00,  1.4091e+00,  ...,  6.7252e-01,\n",
      "          -1.3435e+00, -1.4017e+00],\n",
      "         [ 1.0602e+00,  9.2452e-01,  1.1765e+00,  ...,  1.1378e+00,\n",
      "          -5.4873e-01, -8.2012e-01]],\n",
      "\n",
      "        [[-2.4183e+00, -2.4183e+00, -2.4183e+00,  ..., -2.4183e+00,\n",
      "          -2.4183e+00, -2.4183e+00],\n",
      "         [-1.1989e+00, -1.5136e+00, -1.4742e+00,  ...,  1.7777e-01,\n",
      "           4.0101e-02,  2.0434e-02],\n",
      "         [-2.0249e+00, -2.4183e+00, -2.2609e+00,  ..., -6.8757e-01,\n",
      "          -7.8591e-01, -7.0724e-01],\n",
      "         ...,\n",
      "         [ 1.1611e+00,  5.3177e-01,  4.9244e-01,  ...,  4.1377e-01,\n",
      "          -1.8479e+00, -2.0446e+00],\n",
      "         [ 9.2511e-01,  5.9077e-01,  7.4811e-01,  ...,  1.9744e-01,\n",
      "          -1.8086e+00, -1.7496e+00],\n",
      "         [ 3.1544e-01,  7.6703e-04,  4.1377e-01,  ...,  4.9244e-01,\n",
      "          -1.1989e+00, -1.3759e+00]],\n",
      "\n",
      "        [[-2.2214e+00, -2.2214e+00, -2.2214e+00,  ..., -2.2214e+00,\n",
      "          -2.2214e+00, -2.2214e+00],\n",
      "         [-9.9224e-01, -1.3434e+00, -1.3825e+00,  ..., -1.1428e-01,\n",
      "          -2.3134e-01, -2.1183e-01],\n",
      "         [-1.8312e+00, -2.2214e+00, -2.2214e+00,  ..., -1.1483e+00,\n",
      "          -1.2459e+00, -1.1093e+00],\n",
      "         ...,\n",
      "         [-4.4596e-01, -1.7922e+00, -1.7531e+00,  ..., -5.2400e-01,\n",
      "          -2.0458e+00, -2.0458e+00],\n",
      "         [-3.4841e-01, -1.5580e+00, -1.7141e+00,  ..., -8.5567e-01,\n",
      "          -2.0848e+00, -1.8312e+00],\n",
      "         [-3.4841e-01, -1.4020e+00, -1.6361e+00,  ..., -3.8743e-01,\n",
      "          -1.5580e+00, -1.5580e+00]]])\n"
     ]
    }
   ],
   "source": [
    "data, label = train_data[0]\n",
    "print(type(data))\n",
    "print(data.size())\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4e16d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74780b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f59a350fdd0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(data.permute(2,1,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fed3cd",
   "metadata": {},
   "source": [
    "- in the same way we can transform test data as well. \n",
    "- for example, converting to tensors is the only transform on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d024e013",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "    (0.4914, 0.4822, 0.4465),\n",
    "    (0.2023, 0.1994, 0.2010)\n",
    "    )\n",
    "])\n",
    "\n",
    "test_data = CIFAR10(root='./test/', train=False, transform=test_transforms)\n",
    "\n",
    "print(test_data)\n",
    "\n",
    "print(test_data.transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ec1da0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
