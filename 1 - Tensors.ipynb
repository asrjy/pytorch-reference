{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b402c9d3",
   "metadata": {},
   "source": [
    "- torch.Tensor is the fundamental data structure in pytorch.\n",
    "- it's a data structure used to store and manipulate data.\n",
    "- like a numpy array contains data of same type. \n",
    "- can hold scalars, vectors, matrices and n dimensional arrays\n",
    "- derived from torch.Tensor class. \n",
    "- tensor operations are faster than numpy for gpu acceleration\n",
    "- tensors can be stored and manipulated at scale using distributed processing on multiple cpus and gpus\n",
    "- tensors keep track of their graph computations (autograd) which is a key part of implementing a deep learning library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3a7531",
   "metadata": {},
   "source": [
    "- By default, the tensor data type will be derived from the input data type and the tensor will be allocated to the cpu device. \n",
    "- we can add two tensors directly with + sign as tensors support operator overloading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59d7340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8, 10, 12],\n",
      "        [14, 16, 18]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "y = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "z = x + y\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd080a79",
   "metadata": {},
   "source": [
    "torch.Tensor() is deprecated version of torch.tensor(). torch.Tensor() is alias for torch.FloatTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d424230e",
   "metadata": {},
   "source": [
    "- storage location can be accessed with z.device attribute\n",
    "- torch.cuda.is_available() will return True if device has GPU support. \n",
    "- ouput for torch.cuda.is_available() will tell how many GPUs are availbale and which one to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e834dd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8, 10, 12],\n",
      "        [14, 16, 18]], device='cuda:0')\n",
      "torch.Size([2, 3])\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]], device = device)\n",
    "y = torch.tensor([[7, 8, 9], [10, 11, 12]], device = device)\n",
    "z = x + y\n",
    "print(z)\n",
    "print(z.size())\n",
    "print(z.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103522ba",
   "metadata": {},
   "source": [
    "- it's common to transfer tensor from one device to another \n",
    "- can be done with torch.to() method. \n",
    "- further operations on moved tensor will store results in target device\n",
    "- operations between tensors in different devices will result in errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c2ca51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "z = x+y\n",
    "z = z.to('cpu')\n",
    "print(z.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72478cf",
   "metadata": {},
   "source": [
    "the following are equivalent. they can be used instead of device objects\n",
    "- device = 'cuda'\n",
    "- device = torch.device('cuda')\n",
    "- device = 'cuda:0'\n",
    "- device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e2eb90",
   "metadata": {},
   "source": [
    "- tensors can be created from preexisting numeric data(python data structures, numpy arrays etc.,), create random samplings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1fe9cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "# python list\n",
    "w = torch.tensor([1, 2, 3])\n",
    "# python tuple\n",
    "w = torch.tensor((1, 2, 3))\n",
    "# numpy array\n",
    "w = torch.tensor(numpy.array([1, 2, 3]))\n",
    "# uninitialized\n",
    "w = torch.empty(100, 200)\n",
    "# all elements are 0s and shape is provided\n",
    "w = torch.zeros(100, 200)\n",
    "# all elements are 1s and shape is provided\n",
    "w = torch.ones(100, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe13abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100x200 tensor with elements from uniform distribution on the interval [0, 1)\n",
    "w = torch.rand(100, 200)\n",
    "# 100x200 tensor with elements from normal distribution with mean 0 and variance 1\n",
    "w = torch.randn(100, 200)\n",
    "# 100x200 elements are random integers between 5 and 10\n",
    "w = torch.randint(5, 10, (100, 200))\n",
    "# specifying device and datatype\n",
    "w = torch.empty((100, 200), dtype = torch.float64, device = 'cuda')\n",
    "# intialized to have same size, data type and device as another tensor\n",
    "w = torch.empty_like(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea49510",
   "metadata": {},
   "source": [
    "- ones_like and zeros_like() can also be used to create tensors similar to other tensors. \n",
    "- linspace() can be used to create tensor with linearly spaced points between two points. \n",
    "- logspace() can be used to create tensor with logarithmically spaced points between two points. \n",
    "- eye() can be used to create a tensor with ones on diagonals and zeros everywhere else ie., identity matrix\n",
    "- full() create tensor of specified shape filled with specified value\n",
    "- load() and save() to load from and save to pickle files\n",
    "- torch.numpy() and torch.tolist() to convert tensors to numpy arrays and python lists. \n",
    "- torch.bernoully() draws binnary random numbers from a bernoulli distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96ead7b",
   "metadata": {},
   "source": [
    "- x.dtype to get data type \n",
    "- x.device to get device\n",
    "- x.shape to get shape\n",
    "- x.ndim to get rank aka number of dimensions\n",
    "- x.requires_grad indicates whether the tensor keeps track of graph computations \n",
    "- x.grad contains actual gradients if requires_grad is true\n",
    "- x.grad_fn stores graph computation function used\n",
    "- x.is_cuda, x.is_sparse, x.is_quantized, x.is_leaf, x.is_mkldnn various indicators \n",
    "- x.layout indicates how tensor is laid out in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b723af11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.strided"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1b13444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.int32\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([1, 2, 3], dtype = torch.float32)\n",
    "print(w.dtype)\n",
    "# converting to int\n",
    "w = w.int()\n",
    "print(w.dtype)\n",
    "# to() can be used to convert to other datatypes\n",
    "w = w.to(torch.float64)\n",
    "print(w.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66809ef",
   "metadata": {},
   "source": [
    "- pytorch automatically converts datatypes to appropriate datatypes\n",
    "- to reduce space complexity and to reuse memory and perform inplace operations, append _ to function name. \n",
    "\n",
    "    for example: y.add_(x) adds x to y and stores it again in y. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b5e425",
   "metadata": {},
   "source": [
    "- torch.empty_like() will create empty tensor with dtype, device and layout properties of target tensor. \n",
    "- zeros_like(), full_like(), rand_like(), randn_like(), rand_int_like() also work similarly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fca0e1",
   "metadata": {},
   "source": [
    "- indexing and slicing will return tensor object even if array is only single element. need to use item() to convert single element tensor to a python value when passing to other functions for example print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69e20b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n",
      "tensor(4)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "print(x)\n",
    "print(x[1, 1])\n",
    "print(x[1, 1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c3c66ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 3, 5, 7],\n",
      "        [2, 4, 6, 8]])\n"
     ]
    }
   ],
   "source": [
    "# indexing, slicing all can be done similar to numpy arrays. \n",
    "# transpose \n",
    "print(x.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7d0bcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "# view is preferred over reshape\n",
    "print(x.view((2, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcb160cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4],\n",
      "         [5, 6],\n",
      "         [7, 8]],\n",
      "\n",
      "        [[1, 2],\n",
      "         [3, 4],\n",
      "         [5, 6],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "# tensors can be combined with torch.stack()\n",
    "y = torch.stack((x, x))\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b27d1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 3, 5, 7]) tensor([2, 4, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "# tensors can be split with torch.unbind()\n",
    "a, b = x.unbind(dim = 1)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3e4921",
   "metadata": {},
   "source": [
    "- torch.cat() to concatenate sequences along specified dimension\n",
    "- torch.chunk() to split tensor into specific number of chunks. it only returns a view\n",
    "- torch.narrow() returns narrow version of input tensor\n",
    "- torch.reshape() returns reshaped. use torch.view() to ensure tensor is not copied. \n",
    "- torch.squeeze() returns tensor with all dimensions of input tensor of size 1 removed. used to remove ununsed dimensions. for example convert images from 4d to 3d\n",
    "- torch.unsqueeze() is used to add a dimension of size 1. most pytorch models expect batch of data as input, unsqueeze() helps  when we have only one data sample passing 3d image to torch to create a batch of one image\n",
    "- torch.transpose() only transposes specific dimensions. best for multidimensional tensors\n",
    "- torch.where() returns tensor of selected elements depending on specified condition "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d8135e",
   "metadata": {},
   "source": [
    "- basic math funcs: add(), div(), mul(), neg(), reciprocal(), true_divide()\n",
    "- truncation funcs: ceil(), clamp(), floor(), floor_divide(), fmod(), frac(), lerp(), remainder(), round(), sigmoid(), trunc()\n",
    "- complex num funcs: abs(), angle(), coj(), imga(), real()\n",
    "- trigonometry funcs: acos(), asin(), atan(), cos(), cosh(), deg2rad(), rad2deg(), sin(), sinh(), tan(), tanh()\n",
    "- bitwise op funcs: bitwise_not(), bitwise_and(), bitwise_or(), bitwise_xor()\n",
    "- error funcs: erf(), erfc(), erfinv()\n",
    "- cumulative math funcs: addcdiv(), addcmul()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcb9c84",
   "metadata": {},
   "source": [
    "reduction operations reduce dimensionality or rank of tensor. \n",
    "- torch.argmax() returns indices of maximum values across all elements or a dimensions\n",
    "- torch.dist() returns the p-norm of two tensors \n",
    "- torch.sum() returns sum of all elements or a dimension\n",
    "- torch.unique() removes duplicates across the tensor or a dimension\n",
    "- torch.unique_consecutive() removes consecutive duplicates\n",
    "- lot of these functions require dim parameter. it's similar to axis is numpy\n",
    "- if dim is not specified, operation is performed across all dimensions. ex: dim = 1 will copute operation across each row and so on. \n",
    "\n",
    "- it is common to chain methods together like torch.rand(2, 2).max().item() returns max value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd93c47",
   "metadata": {},
   "source": [
    "comparision functions return tensor of booleans  after comparision\n",
    "- compare tensors with one other: eq(), ge(), gt(), le(), lt() or ==, >, >=, <, <=, != \n",
    "- test tensor status or conditions: isclose(), isfinite(), isinf(), isnan()\n",
    "- return a single boolean for whole tensor - allclose(), equal()\n",
    "- find values over tensor along a dimension - argosrt(), kthvalue(), max(), min(), sort(), topk()\n",
    "- torch.eq() return a boolean tensor and torch.equal() returns a single boolean value\n",
    "- torch.allclose() returns a single value if all elements are close to a specified value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdb0683",
   "metadata": {},
   "source": [
    "many computations such as gradient descent use linear algebra to implement calculations. \n",
    "- pytorch linalg operations are based on Basic Linear Algebra Subprograms (BLAS) and Linear Algebra Package (LAPACK) standardized libraries. \n",
    "- torch.matmul() matrix product of two tensors. supports broadcasting\n",
    "- torch.chain_matmul() computes a matrix product of n tensors\n",
    "- torch.mm() matrix product of two tensors without broadcasting\n",
    "- torch.addmm() matrix product of two tensors and adds it to input\n",
    "- torch.bmm() batch of matrix products\n",
    "- torch.addbmm() computes batch of matrix products and adds it to the input\n",
    "- torch.baddbmm() computes batch and matrix products and adds it to the input batch\n",
    "- torch.mv() computes product of matrix and tensor\n",
    "- torch.addmv() computes product of matrix and vector and adds it to the input\n",
    "- torch.matrix_power() returns tensor raised to power of n (for square tensors)\n",
    "- torch.eig() finds eigen values and eigen vectors of real square tensor\n",
    "- torch.inverse() computes inverse of square tensor\n",
    "- torch.det() determinant of matrix/batch of matrices\n",
    "- torch.solve() returns solution to a system of linear equations \n",
    "- torch.svd() performs single value decomposition\n",
    "- torch.pca_lowrank() performs linear princinple component analysis\n",
    "- torch.cholesky() performs cholesky decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009302fd",
   "metadata": {},
   "source": [
    "- fast, inverse and short time fourier transforms: fft(), ifft(), stft()\n",
    "- histogram and bin counts: histc(), bincount()\n",
    "- windowing algorithms: bartlett_window(), blackman_window(), hamming_window(), hann_window()\n",
    "- matrix reduction and restructuring functions: flatten(), flip(), rot90(), repeat_interleave(), meshgrid(), roll(), combinations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec00e2d",
   "metadata": {},
   "source": [
    "backward() function uses pytorch's automatic differentitation package, torch.autograd, to differentiate and compute gradientes of tensors based on chain rule. this is what makes pytorch powerful for deep learning. \n",
    "\n",
    "defining a function f = sum(x^2) and finding df/dx for each variable in the matrix. in order to do this we set requires_grad = True for tensor x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8006ac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype = torch.float, requires_grad = True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67c11d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(91., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "f = x.pow(2).sum()\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3978e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  4.,  6.],\n",
      "        [ 8., 10., 12.]])\n"
     ]
    }
   ],
   "source": [
    "f.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70f739d",
   "metadata": {},
   "source": [
    "f.backward() performs differentiation wrt f and stores it in x.grad attribute. \n",
    "\n",
    "diff of x^2 = 2x and x.grad contains all elements multiplied with 2\n",
    "\n",
    "training neural networks requires us to compute weight gradients on backward pass. as our nns get deeper and more complex, feature automates the complex computations. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
